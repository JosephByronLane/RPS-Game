{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX8mhOLljYeM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "BZSlp3DAjdYf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Use como base el archivo que provee Tensorflow en su video de \"Build an image classifier (ML Zero to Hero - Part 4)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Termine agregando soporte para GPU en tensorflow, cambie la red neuronal (creo, la cambie varias veces y no se si termine haciendo un circulo completo)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> la verdad no se exactamente cuanto cambie, pero si le cambie bastantito</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "True\n",
      "True\n",
      "gpu #:  4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_built_with_gpu_support())\n",
    "print(\"gpu #: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "gpu num: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('gpu num: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Click here if you'd like to modify the basic game functionality</h1> <br> <p>If not, simply run all cells</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The amount of time you need to have your thumb up/down to activate/deactivate the game\n",
    "thumbs_duration = 2\n",
    "\n",
    "##Total amount of turns to play against the comupter\n",
    "total_turns = 5\n",
    "\n",
    "##The amount of time you have to choose a move.\n",
    "countdown_time = 5\n",
    "\n",
    "##The amount of wait between turns. (When it shows you what the computer chose and who won)\n",
    "wait_between_turns = 2\n",
    "\n",
    "##The amount of time after a game ends (When it shows you your and the computer scores, and shows who won)\n",
    "wait_after_game_end=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Click here if you'd like to modify the advanced parts of the code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Global settings######\n",
    "\n",
    "#Saved model directory (Already created/trained) \n",
    "model_dir = 'rpsmodels'\n",
    "\n",
    "#Training-validation data split (0.2 means 20% validation, 80% training)\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Training settings.######\n",
    "\n",
    "#Set to true if you'd like to generate (train) a new model\n",
    "generate_new_model = True\n",
    "\n",
    "epochs = 25\n",
    "steps_per_epoch = 25\n",
    "\n",
    "##Dataset Input directory\n",
    "input_dir = r'RPS\\rps-custom-sanitized-augmented-v2'\n",
    "\n",
    "##Dataset 'type'. Is only used for the sake of knowing what dataset type a model was trained with\n",
    "data_type = 'test'\n",
    "\n",
    "#The size of the training images. Higher retains more detail but consumes *alot* more memory\n",
    "image_size  = 150\n",
    "\n",
    "#The amount of images to use per training step. Higher number = more memory.\n",
    "batch_size = 512\n",
    "\n",
    "#If theres validation during training. Recommended to leave at True.s\n",
    "val=True\n",
    "\n",
    "#Amount of validation steps after each epoch.\n",
    "val_step = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Augmentation settings######\n",
    "##These only matter if you also have generate_new_model = True###\n",
    "\n",
    "\n",
    "#Set to true if you'd like to also generate augmented images.\n",
    "#If set to true, it will cache the images, if set to false it will augment imaages during training.\n",
    "\n",
    "make_aug_images = False\n",
    "\n",
    "#Augmented image output directory.\n",
    "output_dir = r'RPS\\rps-custom-sanitized-augmented-v4'\n",
    "\n",
    "#Number of workers for image augmentation. Recommended is core count/2\n",
    "aug_image_workers = int(os.cpu_count()/2)\n",
    "\n",
    "#Number of augmented images to make per image.\n",
    "num_augmented_images_per_input = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model finding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_files = []\n",
    "'rpsmodels/rps-custom11-20231214-25-25-VAL-150-512-5-0.2-sanitized.h5'\n",
    "try:\n",
    "    for filename in os.listdir(model_dir):    \n",
    "        if filename.endswith('.h5'):\n",
    "            full_path = os.path.join(model_dir, filename)\n",
    "            creation_date = os.path.getctime(full_path)\n",
    "            model_number = None\n",
    "    \n",
    "            parts = filename.split('-')\n",
    "            for part in parts:\n",
    "                if part.startswith('custom'):\n",
    "                    try:\n",
    "                        model_number = int(part[6:])  \n",
    "                    except ValueError:\n",
    "                        model_number = None\n",
    "                    break\n",
    "    \n",
    "            model_files.append((full_path, creation_date, model_number))\n",
    "    \n",
    "    model_files.sort(key=lambda x: x[1])\n",
    "    sorted_filenames = [item[0] for item in model_files]\n",
    "    sorted_model_numbers = [item[2] for item in model_files if item[2] is not None]\n",
    "    \n",
    "    model_ver= sorted_model_numbers[len(sorted_model_numbers)-1]\n",
    "    latest_model= sorted_filenames[len(sorted_filenames)-1]\n",
    "    print(\"done2\")\n",
    "except:\n",
    "    print(\"Model directory not found, creating one.\")\n",
    "    os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, the game should have launched.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    model = load_model(latest_model)\n",
    "    print(\"All good, the game should have launched.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model not found. Please introduce one into {model_dir} and rerun the notebook. Error: {e}\", file=sys.stderr)\n",
    "    raise Exception(f'Please introduce a model into {model_dir} and rerun the notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Image augmentation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(class_name, image_file):\n",
    "    img_path = os.path.join(input_dir, class_name, image_file)\n",
    "    img = load_img(img_path)\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    img_array = img_array[2:-2, 2:-2, :] #adding this line of code solved 4 hours of debugging. Ask me why.\n",
    "\n",
    "    \n",
    "    img_array = img_array.reshape((1,) + img_array.shape)\n",
    "\n",
    "    output_class_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(img_array, batch_size=1, save_to_dir=output_class_dir, save_prefix='aug', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i >= num_augmented_images_per_input:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from multiprocessing import Pool\n",
    "from augment_image import worker\n",
    "\n",
    "\n",
    "if(make_aug_images):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=60,\n",
    "        brightness_range=[0.6, 1.4],  \n",
    "        channel_shift_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='reflect',\n",
    "        validation_split=VALIDATION_SPLIT  \n",
    "    )\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "   \n",
    "    tasks = []\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_dir = os.path.join(input_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            tasks.append((class_name, image_file, input_dir, output_dir, datagen, num_augmented_images_per_input))   \n",
    "    \n",
    "    with Pool(processes=aug_image_workers) as pool:  \n",
    "        pool.map(worker, tasks) \n",
    "    \n",
    "    ##overwrite with a simpler datagen now that the images hae been augmented.\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=VALIDATION_SPLIT\n",
    "    )\n",
    "    training_dir = output_dir\n",
    "\n",
    "else:    \n",
    "    training_dir = input_dir\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=60,\n",
    "        brightness_range=[0.6, 1.4],  \n",
    "        channel_shift_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='reflect',\n",
    "        validation_split=VALIDATION_SPLIT  \n",
    "    )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Datagen creation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13161 images belonging to 5 classes.\n",
      "Found 3288 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  \n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model creation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,474,501\n",
      "Trainable params: 3,474,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if (generate_new_model):\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(image_size, image_size, 3)),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(5, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWTisYLQM1aM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.8198 - accuracy: 0.2163"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "validation_used = 'VAL' if val else 'NOVAL'\n",
    "\n",
    "if generate_new_model:   \n",
    "    if (val):\n",
    "        \n",
    "        history = model.fit(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch,validation_data = validation_generator, verbose = 1, validation_steps=val_step)\n",
    "    else:\n",
    "        history = model.fit(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose = 1)\n",
    "\n",
    "    model_ver= model_ver + 1\n",
    "    \n",
    "    current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True) \n",
    "\n",
    "\n",
    "    model_filename = f'rps-custom{model_ver}-{current_date}-{epochs}-{steps_per_epoch}-{validation_used}-{image_size}-{batch_sizee}-{num_augmented_images_per_input}-{VALIDATION_SPLIT}-{data_type}.h5'\n",
    "    latest_model = os.path.join(model_dir, model_filename)\n",
    "    \n",
    "    model.save(latest_model)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aeTRVCr6aosw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model generated. Cant retrieve validation data.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if(generate_new_model):        \n",
    "    if(val):\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        \n",
    "        epochs = range(len(acc))\n",
    "        \n",
    "        plt.figure(facecolor='#181a1b')\n",
    "        plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "        plt.title('Training and validation accuracy', color='White')\n",
    "        plt.tick_params(colors='white')\n",
    "        plt.legend(loc=0)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor('#181a1b')\n",
    "        ax.spines['bottom'].set_color('white')\n",
    "        ax.spines['top'].set_color('white')\n",
    "        ax.spines['left'].set_color('white')\n",
    "        ax.spines['right'].set_color('white')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        acc = history.history['accuracy']\n",
    "        loss = history.history['loss']\n",
    "        \n",
    "        epochs = range(len(acc))\n",
    "        \n",
    "        plt.figure(facecolor='#181a1b')\n",
    "        plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "        plt.title('Training accuracy', color='White')\n",
    "        plt.tick_params(colors='white')\n",
    "        plt.legend(loc=0)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor('#181a1b')\n",
    "        ax.spines['bottom'].set_color('white')\n",
    "        ax.spines['top'].set_color('white')\n",
    "        ax.spines['left'].set_color('white')\n",
    "        ax.spines['right'].set_color('white')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No model generated. Cant retrieve validation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy as of model rps-custom9-20231213-25-25-VAL-150-128-2-0.2-combined</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/77eqbyb.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZABJp7T3VLCU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'rock', 'scissors', 'thumbsdown', 'thumbsup']\n",
      "Validation of model rpsmodels\\rps-custom11-20231214-25-25-VAL-150-512-5-0.2-sanitized.h5\n",
      "paper (39).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (141).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (40).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (117).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (115).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (195).jpg: [[0. 1. 0. 0. 0.]]\n",
      "paper (132).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (86).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (138).jpg: [[1. 0. 0. 0. 0.]]\n",
      "paper (102).jpg: [[1. 0. 0. 0. 0.]]\n",
      "rock (109).jpg: [[0. 0. 0. 1. 0.]]\n",
      "rock (1).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (89).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (88).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (120).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (67).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (112).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (59).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (33).jpg: [[0. 1. 0. 0. 0.]]\n",
      "rock (184).jpg: [[0. 1. 0. 0. 0.]]\n",
      "scissors (13).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (85).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (93).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (118).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (157).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (167).jpg: [[1. 0. 0. 0. 0.]]\n",
      "scissors (41).jpg: [[1. 0. 0. 0. 0.]]\n",
      "scissors (21).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (103).jpg: [[0. 0. 1. 0. 0.]]\n",
      "scissors (107).jpg: [[0. 0. 1. 0. 0.]]\n",
      "thumbsdown (49).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (67).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (2).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (118).jpg: [[0.28 0.   0.   0.72 0.  ]]\n",
      "thumbsdown (196).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (154).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (194).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (22).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (139).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsdown (89).jpg: [[0. 0. 0. 1. 0.]]\n",
      "thumbsup (52).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (124).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (149).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (160).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (171).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (69).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (79).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (49).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (194).jpg: [[0. 0. 0. 0. 1.]]\n",
      "thumbsup (187).jpg: [[0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "try:\n",
    "    model = load_model(latest_model)\n",
    "except Exception as e:\n",
    "    print(f\"Model not found. Please introduce one into {model_dir} and rerun the notebook. Error: {e}\", file=sys.stderr)\n",
    "    raise Exception(f'Please introduce a model into {model_dir} and rerun the notebook.')\n",
    "\n",
    "\n",
    "base_dir = 'RPS/rps-custom-validation/'\n",
    "classes = ['paper', 'rock', 'scissors', 'thumbsdown', 'thumbsup']\n",
    "print(classes)\n",
    "print(f'Validation of model {latest_model}')\n",
    "\n",
    "def load_and_preprocess_image(file_path):\n",
    "    img = image.load_img(file_path, target_size=(image_size, image_size))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) \n",
    "    return img_array\n",
    "    \n",
    "for class_name in classes:\n",
    "    dir_path = os.path.join(base_dir, class_name)\n",
    "    file_list = os.listdir(dir_path)\n",
    "    \n",
    "    sampled_files = random.sample(file_list, 10)\n",
    "    \n",
    "    for file_name in sampled_files:\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        \n",
    "        img_array = load_and_preprocess_image(file_path)\n",
    "        \n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        prediction_rounded = np.round(prediction, decimals=2) \n",
    "        \n",
    "        print(f'{file_name}: {prediction_rounded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The game itself </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Pre-requisite functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame_rgb=frame\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #i hate color\n",
    "\n",
    "    frame_resized = cv2.resize(frame_rgb, (image_size, image_size))\n",
    "\n",
    "    frame_normalized = frame_resized / 255.0\n",
    "\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)\n",
    "\n",
    "    return frame_expanded, frame_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_winner(user_move, computer_move):\n",
    "    if user_move == computer_move:\n",
    "        return 'Draw'\n",
    "    elif (user_move == 'rock' and computer_move == 'scissors') or \\\n",
    "         (user_move == 'scissors' and computer_move == 'paper') or \\\n",
    "         (user_move == 'paper' and computer_move == 'rock'):\n",
    "        return 'User wins'\n",
    "    else:\n",
    "        return 'Computer wins'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialization data.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width: 2560\n",
      "height: 1440\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import tkinter as tk\n",
    "import subprocess\n",
    "import pyautogui\n",
    "import random\n",
    "\n",
    "moves = ['paper', 'rock', 'scissors']\n",
    "try:\n",
    "    model = tf.keras.models.load_model(latest_model)\n",
    "except Exception as e:\n",
    "    print(f\"Model not found. Please introduce one into {model_dir} and rerun the notebook. Error: {e}\", file=sys.stderr)\n",
    "    raise Exception(f'Please introduce a model into {model_dir} and rerun the notebook.')\n",
    "\n",
    "\n",
    "x_start = 100\n",
    "y_start = 100\n",
    "width = 900\n",
    "height = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The game itself, for real this time</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.399304   0.01592811 0.27209127 0.28719157 0.02548505]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prediction_counter = 0\n",
    "game_active = False\n",
    "thumbs_up_time = None\n",
    "thumbs_down_time = None\n",
    "user_score = computer_score = 0\n",
    "turn_ammount = 1\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    screen_capture = pyautogui.screenshot(region=(x_start, y_start, width, height))\n",
    "    frame = np.array(screen_capture)\n",
    "    frame_to_show = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "    \n",
    "    preprocessed_frame, model_sees = preprocess_frame(frame_to_show)\n",
    "    #cv2.imshow('Model view', model_sees)\n",
    "    \n",
    "    prediction = model.predict(preprocessed_frame, verbose=0)\n",
    "    print(prediction)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    user_move = classes[np.argmax(prediction)]\n",
    "    cv2.putText(frame_to_show, f'User: {user_move}', (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 0), 3)\n",
    "\n",
    "\n",
    "    if user_move == 'thumbsup':\n",
    "        if thumbs_up_time is None:\n",
    "            thumbs_up_time = time.time()  \n",
    "        elif time.time() - thumbs_up_time > thumbs_duration:\n",
    "            game_active = True\n",
    "            start_time = time.time()\n",
    "    else:\n",
    "        thumbs_up_time = None \n",
    "\n",
    "    if user_move == 'thumbsdown':\n",
    "        start_time = time.time()\n",
    "\n",
    "        if thumbs_down_time is None:\n",
    "            thumbs_down_time = time.time() \n",
    "        elif time.time() - thumbs_down_time > thumbs_duration:\n",
    "            game_active = False\n",
    "\n",
    "    else:\n",
    "        thumbs_down_time = None  \n",
    "\n",
    "    if not game_active:            \n",
    "        cv2.putText(frame_to_show, f'GAME INACTIVE', (350, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "\n",
    "    \n",
    "    if game_active:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        countdown_text = str(5 - int(elapsed_time)) \n",
    "\n",
    "        cv2.putText(frame_to_show, countdown_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_to_show, f'GAME ACTIVE', (125, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "        \n",
    "        if elapsed_time > countdown_time:\n",
    "    \n",
    "            prediction_counter += 1\n",
    "    \n",
    "            computer_move = random.choice(moves)\n",
    "            \n",
    "            result = determine_winner(user_move, computer_move)\n",
    "            if result == \"User wins\":\n",
    "                user_score += 1\n",
    "            elif result == \"Computer wins\":\n",
    "                computer_score += 1\n",
    "            \n",
    "            cv2.putText(frame_to_show, f'Computer: {computer_move}', (500, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "            if(result == \"User wins\"):\n",
    "                cv2.putText(frame_to_show, f'Result: {result}', (175, 650), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 255, 0),5)\n",
    "            else:\n",
    "                cv2.putText(frame_to_show, f'Result: {result}', (175, 650), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 0, 255),5)\n",
    "            \n",
    "            print(f'Prediction #{prediction_counter}: User: {user_move}, Computer: {computer_move}, Result: {result}')\n",
    "            #cv2.imshow('Model Input', model_sees)\n",
    "            cv2.imshow('Screen Capture', frame_to_show)\n",
    "\n",
    "            cv2.waitKey(wait_between_turns*1000) \n",
    "            \n",
    "            turn_ammount +=1;\n",
    "            if(turn_ammount > total_turns):\n",
    "                screen_capture = pyautogui.screenshot(region=(x_start, y_start, width, height))\n",
    "                framen = np.array(screen_capture)\n",
    "                frame_new = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   \n",
    "\n",
    "                result_text = f'User: {user_score} | Computer: {computer_score}'\n",
    "\n",
    "\n",
    "                winner = 'User' if user_score > computer_score else 'Computer' if computer_score > user_score else 'Tie'\n",
    "                cv2.putText(frame_new, result_text, (195, 450), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 0), 3)\n",
    "                cv2.putText(frame_new, f'Winner: {winner}', (195, 550), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 0), 3)\n",
    "                cv2.imshow('Screen Capture', frame_new)\n",
    "                cv2.waitKey(wait_after_game_end*1000)\n",
    "                game_active = False\n",
    "                user_score = computer_score = 0\n",
    "                turn_ammount = 1\n",
    "                start_time = current_time+wait_after_game_end\n",
    "\n",
    "                continue\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            start_time = current_time+wait_between_turns\n",
    "    cv2.imshow('Screen Capture', frame_to_show)\n",
    "    cv2.setWindowProperty('Screen Capture', cv2.WND_PROP_TOPMOST, 1)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Troubleshooting</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3>It says im missing a model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Make sure you have a model loaded in a folder called 'rpsmodels' in the same directory as this notebook.</p>\n",
    "<p>Ej, if this notebook is in C:/Rock paper scissors.ipynb then make sure theres a trained model in C:/rpsmodels/model_name.h5</p>\n",
    "<p>If you dont have one, re-download where this  file game from. It should come with one bundled in.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3>My CPU/GPU ramps up and nothing happens</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Make sure to delve into the advanced parts of the code and set generate_new_model and make_aug_images to false.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ive waited over 10 minutes and nothing happens.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Go through the advanced parts of the code, opening each tab until you see red markings which sream errors at you. Scroll to the bottom of them and copy&paste the error into ChatGPT or google.</p>\n",
    "<p>Its very possible you're simply missing a library.</p>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
